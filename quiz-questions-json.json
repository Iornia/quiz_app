[
  {
    "question": "AI is often defined in multiple ways, but which statement best captures its essence?",
    "options": [
      "AI has been defined in different ways, focusing on human-like intelligence vs. rationality and thought processes vs. behavior.",
      "AI is purely a branch of computer engineering and does not involve psychology or mathematics.",
      "AI strictly aims to mimic human intelligence, ignoring logical reasoning and decision-making.",
      "AI is a narrowly defined field with a single research approach."
    ],
    "answer": 0,
    "explanation": "人工智能(AI)有多種定義方式，關鍵在於其多元特性。正確的定義涵蓋了AI研究的兩個主要維度：人類智能與理性思維的對比，以及思維過程與行為表現的對比。這反映了AI領域的廣泛性和多樣性，而非單一、狹隘的定義。",
    "chapter": "AI Introduction"
  },
  {
    "question": "Which of the following is not a required capability for a computer to pass the Turing Test?",
    "options": [
      "Computer vision and speech recognition to perceive the world.",
      "Natural language processing to communicate successfully in a human language.",
      "Knowledge representation to store information and reasoning to draw conclusions.",
      "Physical simulation of a human body to mimic human motion."
    ],
    "answer": 3,
    "explanation": "圖靈測試的核心是評估機器是否能夠表現出與人類相當的智能，主要通過語言溝通能力來判斷。根據圖靈的設計，物理模擬人類身體並非通過測試的必要條件。測試關注的是智能表現，而不是物理形態的模仿。其他選項如自然語言處理、知識表示與推理、以及感知能力都是通過圖靈測試所需的核心能力。",
    "chapter": "AI Introduction"
  },
  {
    "question": "Which of the following is not a method used to study human thought in cognitive modeling?",
    "options": [
      "Introspection—analyzing one's own thoughts as they occur.",
      "Brain imaging—observing brain activity during cognitive processes.",
      "Psychological experiments—studying human behavior under controlled conditions.",
      "Programming AI solely based on its problem-solving success without comparison to human behavior."
    ],
    "answer": 3,
    "explanation": "認知建模的主要目的是模擬人類思維過程，而不僅僅是解決問題的能力。因此，認知建模必須基於對人類思維過程的研究，包括內省、腦成像和心理實驗等方法。選項D描述的方法只關注AI的問題解決能力，而不進行與人類行為的比較，這與認知建模的核心理念相違背。認知建模的目標是要理解和模擬人類的思維過程，而不僅僅是達到相同的結果。",
    "chapter": "AI Introduction"
  },
  {
    "question": "What is a key limitation of using logic as the foundation for artificial intelligence?",
    "options": [
      "Logic requires certainty, but real-world knowledge is often uncertain.",
      "Logical reasoning cannot be used to model human intelligence.",
      "The theory of probability cannot be integrated with logic.",
      "Logical AI can only solve problems in mathematics and cannot be used for decision-making."
    ],
    "answer": 0,
    "explanation": "邏輯作為人工智能基礎的主要限制在於它通常需要確定性的前提和結論。然而，現實世界充滿不確定性，很多知識和情境不能被絕對確定地表達。邏輯系統難以有效處理這種不確定性，而真實世界的決策經常需要在不完全信息的情況下進行。雖然機率理論可以與邏輯結合來處理不確定性（如貝氏網絡），但純邏輯系統在不確定環境中的表現受到限制。",
    "chapter": "AI Introduction"
  },
  {
    "question": "Which of the following best describes a rational agent in AI?",
    "options": [
      "A rational agent always makes correct inferences, as logical reasoning is the sole basis of rationality.",
      "A rational agent acts to achieve the best possible outcome, or the best expected outcome under uncertainty.",
      "A rational agent must perfectly mimic human behavior and cognitive processes to be considered intelligent.",
      "AI agents cannot act rationally in uncertain environments, as perfect rationality is always required."
    ],
    "answer": 1,
    "explanation": "在AI中，理性代理人(rational agent)的核心定義是：根據可獲得的信息採取行動，以達成最佳可能結果，或在不確定性下獲得最佳期望結果。這種理性不要求完美的邏輯推理或模仿人類行為，而是聚焦於目標導向的最優決策。理性代理人可以在不確定環境中運作，通過期望效用最大化來做出決策，不需要完美的確定性。",
    "chapter": "AI Introduction"
  },
  {
    "question": "What is the primary challenge in designing AI systems according to the value alignment problem?",
    "options": [
      "Ensuring AI systems strictly follow the standard model without modification.",
      "Making AI intelligent enough to always achieve its fixed objectives, regardless of human consequences.",
      "Aligning the AI's objectives with human values and preferences, ensuring it understands and adapts to human needs.",
      "Designing AI to act autonomously without the need for human oversight."
    ],
    "answer": 2,
    "explanation": "價值對齊問題(value alignment problem)的核心挑戰在於確保AI系統的目標與人類價值觀和偏好保持一致。這個問題的難點不僅在於技術層面，還在於AI需要理解、學習並適應複雜且多變的人類需求和價值觀。不恰當的目標函數可能導致AI在追求目標時產生意外的有害行為或決策，即使AI本身運作正常。解決價值對齊問題對於開發安全、有益且值得信賴的AI系統至關重要。",
    "chapter": "AI Introduction"
  },
  {
    "question": "Which of the following is not a potential risk associated with AI development?",
    "options": [
      "AI-powered autonomous weapons that can select and eliminate targets without human intervention.",
      "AI-driven surveillance and persuasion techniques that can influence political behavior.",
      "AI improving job opportunities by eliminating bias and ensuring fair economic distribution.",
      "AI-driven cybersecurity threats, including automated phishing and personalized cyberattacks."
    ],
    "answer": 2,
    "explanation": "在列出的選項中，C項是不符合現實的風險描述。雖然AI確實有潛力改善工作機會和減少偏見，但現實中AI系統常常反映並可能放大現有的社會偏見，而非消除它們。AI對就業市場的影響也很複雜，它可能創造新工作機會，但同時也威脅取代某些工作崗位，可能加劇而非減輕經濟不平等。其他選項都是AI發展中被廣泛討論的實際風險。",
    "chapter": "AI Introduction"
  },
  {
    "question": "What does the term \"percept\" refer to in the context of an agent?",
    "options": [
      "The action an agent performs in response to a stimulus",
      "The information an agent receives from its sensors",
      "The rules programmed into an agent for decision-making",
      "The environment in which an agent operates"
    ],
    "answer": 1,
    "explanation": "在代理人(agent)的背景下，「感知(percept)」指的是代理人通過其感測器(sensors)接收到的信息。這是代理人從環境中獲取信息的方式，代表了代理人對外部世界的原始輸入數據。理解感知的概念對於理解代理人如何感知環境、處理信息並作出反應至關重要。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "What is the primary purpose of a performance measure in an AI agent?",
    "options": [
      "To define the agent's internal architecture",
      "To specify the maximum number of actions an agent can take",
      "To evaluate the desirability of an agent's behavior based on outcomes",
      "To determine how many sensors and actuators an agent requires"
    ],
    "answer": 2,
    "explanation": "在AI代理人中，性能度量(performance measure)的主要目的是評估代理人行為的理想程度，基於其行動所產生的結果。它提供了一個客觀標準，用於判斷代理人的成功與否，並引導代理人朝著預期目標行動。性能度量實質上定義了代理人的「目標」或「效用函數」，是評估代理人理性程度的關鍵。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "Which of the following factors does NOT directly influence an agent's rationality?",
    "options": [
      "The agent's prior knowledge of the environment",
      "The agent's percept sequence to date",
      "The complexity of the agent's code",
      "The performance measure that defines success"
    ],
    "answer": 2,
    "explanation": "代理人的理性行為取決於其能獲得的信息（包括先驗知識和感知序列）以及用於評估其行為的性能度量。代理人代碼的複雜性本身與理性無直接關係—一個簡單的代理人可能在特定環境中表現得非常理性，而一個複雜的代理人可能做出非理性決策。理性關注的是代理人做出最佳行動的能力，而不是實現這種能力的代碼複雜程度。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "Why is omniscience not a requirement for a rational agent?",
    "options": [
      "Because omniscience is impossible in real-world applications",
      "Because rational agents only need to act randomly to perform well",
      "Because agents do not need to perceive their environment to be effective",
      "Because omniscience requires an infinite percept sequence"
    ],
    "answer": 0,
    "explanation": "理性代理人不需要全知全能(omniscience)，因為在實際應用中，全知是不可能實現的。理性代理人只需根據當前可獲得的信息做出最佳決策即可。這反映了「有限理性」(bounded rationality)的概念，即在有限的感知、計算能力和時間約束下做出最佳可能的決策。真實世界中的代理人總是面臨信息不完整的情況，但仍然可以通過有效利用可用信息來表現出理性行為。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "What does the PEAS framework stand for in AI agent design?",
    "options": [
      "Percepts, Exploration, Actuators, and Sensors",
      "Performance, Environment, Actuators, and Sensors",
      "Processing, Execution, Actions, and Strategies",
      "Perception, Encoding, Analysis, and Sensing"
    ],
    "answer": 1,
    "explanation": "PEAS框架是設計AI代理人的一種系統方法，代表：Performance（性能度量）- 評估代理人成功與否的標準；Environment（環境）- 代理人運作的外部世界；Actuators（執行器）- 代理人用來執行動作的組件；Sensors（感測器）- 代理人用來感知環境的設備。這個框架提供了一個全面的視角來分析代理人的需求和運作方式，確保設計時考慮所有關鍵因素。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "Which of the following is an example of a dynamic environment?",
    "options": [
      "A chess game without a time limit",
      "A crossword puzzle",
      "A self-driving car navigating traffic",
      "A tic-tac-toe game on a static board"
    ],
    "answer": 2,
    "explanation": "動態環境(dynamic environment)是指隨時間變化的環境，即使代理人不採取行動。自駕車在交通中導航是動態環境的典型例子，因為交通狀況、其他車輛、行人等都會隨時間變化，無論自駕車是否做出行動。相比之下，無時間限制的棋類遊戲、填字遊戲或靜態棋盤上的井字遊戲都是靜態環境，因為它們只會因代理人的行動而改變，而不會自行變化。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "What is the primary benefit of learning in an AI agent?",
    "options": [
      "It allows the agent to generate new actions randomly.",
      "It helps the agent improve performance based on past experiences.",
      "It removes the need for sensors in the agent.",
      "It ensures the agent always selects the correct action without errors."
    ],
    "answer": 1,
    "explanation": "學習的主要好處是讓代理人能夠根據過去的經驗改進其性能。通過學習，代理人可以從經驗中獲取知識，調整其行為模式，以便在未來的類似情況下做出更好的決策。這使代理人能夠適應不斷變化的環境，處理新的情況，並隨著時間推移提高其效能。學習機制使代理人不必依賴完全預編程的規則，而是可以發展出應對各種情境的能力。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "Why is autonomy important in an AI agent?",
    "options": [
      "It ensures that the agent only follows pre-programmed instructions without change.",
      "It allows the agent to operate without requiring prior knowledge from the designer.",
      "It limits the agent's ability to adapt to new environments.",
      "It prevents the agent from making decisions independently."
    ],
    "answer": 1,
    "explanation": "自主性(autonomy)對AI代理人的重要性在於它使代理人能夠在沒有設計者提供所有先驗知識的情況下運作。具有自主性的代理人可以根據自己的感知和學習來彌補知識缺口，適應新情況，並在環境變化時做出有效決策。這使代理人在設計者無法預見所有可能情況的複雜環境中特別有價值。自主性使代理人更加靈活、適應性強，能夠處理更多樣化的任務和挑戰。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "Which of the following is NOT a part of the PEAS framework for an automated taxi driver?",
    "options": [
      "Sensors: Cameras, GPS, speedometers",
      "Actuators: Steering, brakes, voice synthesizer",
      "Environment: Road conditions, traffic, weather",
      "Intelligence: AI decision-making algorithms"
    ],
    "answer": 3,
    "explanation": "在PEAS框架中，「Intelligence」(智能)或「AI决策算法」不是框架的組成部分。PEAS專注於Performance(性能度量)、Environment(環境)、Actuators(執行器)和Sensors(感測器)四個要素。對於自動駕駛出租車，這些要素包括：安全、乘客滿意度等性能指標；道路、交通和天氣等環境因素；方向盤、剎車等執行器；以及攝像頭、GPS等感測器。智能決策算法是實現代理人功能的內部機制，不屬於PEAS框架範疇。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "Which property describes an environment where the next state is completely determined by the current state and the agent's action?",
    "options": [
      "Partially Observable",
      "Dynamic",
      "Deterministic",
      "Stochastic"
    ],
    "answer": 2,
    "explanation": "確定性(Deterministic)環境是指下一個狀態完全由當前狀態和代理人的行動決定的環境。在這種環境中，相同的狀態和行動總是導致相同的結果，沒有不確定性或隨機性。確定性是環境的一個重要特性，它使得代理人可以準確預測其行動的結果，從而更容易進行計劃和決策。相比之下，隨機(Stochastic)環境中，即使相同的狀態和行動也可能導致不同的結果。",
    "chapter": "Intelligent Agents"
  },
  {
    "question": "What is the primary goal of machine learning in the context of learning from examples?",
    "options": [
      "To explicitly program all rules for decision-making",
      "To develop models based on past data and improve predictions",
      "To replace human intelligence entirely",
      "To store all possible outcomes in memory"
    ],
    "answer": 1,
    "explanation": "從例子學習的機器學習主要目標是基於過去的數據開發模型並改進預測。這種方法的核心是讓算法從數據中學習模式和關係，而不是依賴明確編程的規則。系統通過分析已知的例子(訓練數據)來生成能夠對新數據做出準確預測的模型。這種學習方法使系統能夠處理複雜的問題，適應新情況，並隨著更多數據的獲取而持續改進。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "Which of the following is an example of machine learning from examples?",
    "options": [
      "A calculator performing arithmetic operations",
      "A car learning to recognize stop signs based on images",
      "A fixed set of instructions for sorting a list",
      "A weather forecast that never updates"
    ],
    "answer": 1,
    "explanation": "從例子中學習的典型案例是「車輛基於圖像學習識別停止標誌」。在這個例子中，系統通過分析許多含有停止標誌的標記圖像來學習識別停止標誌的特徵。隨著時間推移，系統通過這些例子學習到停止標誌的模式，然後能夠識別新的、以前未見過的停止標誌圖像。而計算器執行算術運算、固定的排序指令集和不更新的天氣預報都是基於明確規則的系統，不涉及從例子中學習。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "In supervised learning, what is the role of labeled data?",
    "options": [
      "It helps the model learn from input-output pairs",
      "It eliminates the need for training",
      "It is only used in unsupervised learning",
      "It randomly assigns outputs to inputs"
    ],
    "answer": 0,
    "explanation": "在監督學習中，標記數據(labeled data)的作用是提供輸入-輸出對，幫助模型學習。這些標記數據包含了輸入特徵及其相應的「正確答案」(目標輸出)，使算法能夠學習輸入和輸出之間的關係或映射。通過分析這些成對的例子，模型學習到能夠對新的、未見過的輸入進行預測的規則或模式。標記數據是監督學習的核心，因為它提供了學習算法所需的「監督」信號。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "Which of the following is an example of a classification problem in supervised learning?",
    "options": [
      "Predicting tomorrow's temperature",
      "Sorting emails as spam or not spam",
      "Estimating house prices based on square footage",
      "Calculating the average score of a test"
    ],
    "answer": 1,
    "explanation": "分類問題是監督學習的一種類型，其目標是將輸入數據分配到預定義的離散類別中。「將電子郵件分類為垃圾郵件或非垃圾郵件」是典型的二元分類問題，系統需要學習將每封電子郵件歸類為兩個可能類別之一。相比之下，預測溫度和估計房價是回歸問題（預測連續值），而計算測試平均分數則是簡單的統計計算，不涉及預測或學習模式。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "Which method is commonly used to split data into training and testing sets in supervised learning?",
    "options": [
      "Clustering",
      "k-Fold Cross Validation",
      "Random Guessing",
      "Reinforcement Learning"
    ],
    "answer": 1,
    "explanation": "k折交叉驗證(k-Fold Cross Validation)是監督學習中常用的數據分割方法，它將數據集分成k個大小相等的子集，然後進行k次訓練和測試。每次選擇一個不同的子集作為測試集，其餘子集用於訓練。這種方法有效利用有限的數據，確保每個數據點都被用作訓練和測試，提供更可靠的模型性能評估。其他選項如聚類是無監督學習方法，隨機猜測不是有效的數據分割策略，而強化學習是完全不同的機器學習範式。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "Which of the following is NOT a type of supervised learning?",
    "options": [
      "Regression",
      "Classification",
      "Clustering",
      "Decision Trees"
    ],
    "answer": 2,
    "explanation": "聚類(Clustering)不是監督學習的類型，而是無監督學習的一種形式。聚類算法試圖在沒有預定義標籤的情況下，根據數據點之間的相似性將它們分組。相比之下，回歸和分類都是監督學習的類型，分別用於預測連續值和離散類別。決策樹是一種可用於監督學習（分類或回歸）的算法，而不是學習類型本身。聚類與監督學習的根本區別在於它不使用帶標籤的訓練數據。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "In a decision tree, what does an internal node represent?",
    "options": [
      "A final decision",
      "A test on an attribute",
      "A leaf node",
      "A random choice"
    ],
    "answer": 1,
    "explanation": "在決策樹中，內部節點代表對屬性（特徵）的測試。這些測試通常是基於特徵值的條件判斷，例如「年齡>30？」或「顏色是紅色嗎？」。根據測試結果，決策樹將沿著相應的分支繼續。內部節點是決策樹進行決策過程的關鍵部分，它們將數據集分割成越來越小的子集，最終導向包含最終決策（分類標籤或預測值）的葉節點。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "What is the primary purpose of pruning in decision tree learning?",
    "options": [
      "To add more branches to improve accuracy",
      "To simplify the tree and reduce overfitting",
      "To ensure the tree memorizes all training examples",
      "To increase the number of features"
    ],
    "answer": 1,
    "explanation": "在決策樹學習中，修剪(pruning)的主要目的是簡化樹結構並減少過擬合(overfitting)。過擬合發生在模型過於複雜，「記住」了訓練數據的噪音和特殊性，而無法很好地泛化到新數據上。修剪通過移除樹中不必要的分支或子樹，降低模型的複雜度，提高其在未見數據上的泛化能力。這是平衡模型複雜度和預測能力的重要技術，有助於創建更穩健、更可靠的決策樹模型。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "What type of output does a decision tree classification model provide?",
    "options": [
      "A probability distribution over possible values",
      "A continuous numerical value",
      "A discrete class label",
      "A random number"
    ],
    "answer": 2,
    "explanation": "決策樹分類模型的輸出是離散的類別標籤。當一個新的數據點通過決策樹的各個判斷節點後，最終會到達一個葉節點，該葉節點與一個特定的類別相關聯——這就是模型的預測結果。例如，在垃圾郵件分類的情境中，決策樹可能輸出「垃圾郵件」或「非垃圾郵件」這樣的離散標籤。雖然一些決策樹實現也可以提供類別概率，但基本輸出是離散的類別標籤，而非連續值或概率分布。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "Which of the following problems is difficult for a decision tree to represent concisely?",
    "options": [
      "Predicting whether a customer will buy a product",
      "Predicting the majority function (more than half inputs are true)",
      "Diagnosing a medical condition based on symptoms",
      "Identifying spam emails"
    ],
    "answer": 1,
    "explanation": "「預測多數函數」（即判斷超過一半輸入為真）對決策樹來說很難簡潔地表示。這是因為決策樹需要檢查幾乎所有可能的輸入組合才能做出準確預測，導致樹結構變得非常龐大。具體來說，表示n個輸入的多數函數需要一個大小為2^n的決策樹，這對於較大的n值是難以管理的。相比之下，客戶購買預測、醫療診斷和垃圾郵件識別等問題通常可以通過檢查關鍵特徵的決策樹相對簡潔地表示。",
    "chapter": "Learning From Examples"
  },
  {
    "question": "What does a linear combination involve?",
    "options": [
      "Only addition of terms",
      "Only multiplication of terms",
      "Multiplication of values with weights and then summation",
      "Division of values by weights"
    ],
    "answer": 2,
    "explanation": "線性組合(linear combination)是將各項值與權重相乘，然後將這些乘積相加的運算。數學上，它可以表示為：a₁x₁ + a₂x₂ + ... + aₙxₙ，其中x₁...xₙ是值，a₁...aₙ是相應的權重。線性組合是機器學習和數學中許多算法的基礎，包括線性回歸、神經網絡等。它提供了一種將多個輸入值組合成單一輸出的方法，權重決定了每個輸入對最終結果的影響程度。",
    "chapter": "Technical Background"
  },
  {
    "question": "Which mathematical operation best represents the concept of a weighted sum?",
    "options": [
      "Subtraction",
      "Exponentiation",
      "Dot product",
      "Modulus"
    ],
    "answer": 2,
    "explanation": "點積(dot product)最能代表加權和的概念。點積是兩個向量對應元素相乘然後求和的運算，可表示為 a·b = a₁b₁ + a₂b₂ + ... + aₙbₙ。這正好對應了加權和的定義：將每個值與對應的權重相乘，然後求和。點積在機器學習中廣泛應用，例如計算特徵向量與權重向量的加權和，是線性模型、神經網絡等算法的核心運算。",
    "chapter": "Technical Background"
  },
  {
    "question": "How is a dot product mathematically represented?",
    "options": [
      "∑_i▒q_i +c_i",
      "∑_i▒q_i  c_i",
      "∑_i▒q_i /c_i",
      "q.c+∑_i"
    ],
    "answer": 1,
    "explanation": "點積在數學上用 ∑_i q_i * c_i 表示，即對向量的對應元素進行相乘，然後求和。對於兩個向量 q = [q₁, q₂, ..., qₙ] 和 c = [c₁, c₂, ..., cₙ]，它們的點積計算為：q·c = q₁c₁ + q₂c₂ + ... + qₙcₙ。這種運算在線性代數中非常基本，是計算向量投影、相似度以及許多機器學習算法中的加權和的基礎。",
    "chapter": "Technical Background"
  },
  {
    "question": "What happens when np.dot(col_vec, oned_vec) is executed and the shapes are (5,1) and (5,)?",
    "options": [
      "The operation succeeds and returns a (5,1) matrix", 
      "The operation fails due to shape mismatch", 
      "NumPy automatically reshapes the input", 
      "The operation returns a (5,) array"
    ],
    "answer": 1,
    "explanation": "這個問題討論的是NumPy中點積運算的形狀匹配問題。當我們執行np.dot(col_vec, oned_vec)，其中col_vec的形狀是(5,1)（列向量）而oned_vec的形狀是(5,)（一維數組），這個操作會失敗。因為在NumPy中，點積操作要求第一個數組的列數必須等於第二個數組的行數。在這個案例中，col_vec是5x1的矩陣，而oned_vec是被視為1x5矩陣的一維數組，所以形狀不匹配，導致操作失敗。",
    "chapter": "Technical Background"
  },
  {
    "question": "How can we fix a shape mismatch error in np.dot(oned_vec, col_vec)? Given Shapes Before Operation col_vec has shape (5,1) → Column vector oned_vec has shape (5,) → 1D array (treated as row vector)",
    "options": [
      "Reshape col_vec to (5,)", 
      "Reshape oned_vec to (5,1)", 
      "Reshape oned_vec to (1,5)", 
      "Both A and B"
    ],
    "answer": 2,
    "explanation": "為了修復np.dot(oned_vec, col_vec)中的形狀不匹配錯誤，我們需要將oned_vec重塑為(1,5)。一維數組oned_vec形狀為(5,)，在點積運算中被視為行向量（隱含形狀為1x5）。而col_vec是一個列向量，形狀為(5,1)。根據矩陣乘法規則，第一個矩陣的列數必須等於第二個矩陣的行數。通過將oned_vec明確重塑為(1,5)的行向量，我們確保了它的列數（1）與col_vec的行數（1）匹配，從而使點積運算能夠成功執行。",
    "chapter": "Technical Background"
  },
  {
    "question": "What is a weighted average in machine learning?",
    "options": [
      "The simple mean of a dataset", 
      "A sum product where weights sum to one", 
      "The minimum value in the dataset", 
      "The total count of elements in a dataset"
    ],
    "answer": 1,
    "explanation": "在機器學習中，加權平均（weighted average）是指各項數值乘以對應的權重後的總和，且這些權重的總和為1。這與簡單平均不同，簡單平均是所有值的總和除以值的數量，每個值的權重相等。加權平均允許某些數據點比其他數據點具有更大的影響力，在許多機器學習算法中用於各種目的，例如在集成學習中結合多個模型的預測，或在優化過程中更新參數。",
    "chapter": "Technical Background"
  },
  {
    "question": "If a function is represented as f(x,y,z)=x+y+z how many input variables does it have?",
    "options": [
      "1", 
      "2", 
      "3", 
      "4"
    ],
    "answer": 2,
    "explanation": "函數f(x,y,z)=x+y+z有3個輸入變量，分別是x、y和z。函數的輸入變量是指函數依賴的變量，在函數符號中表示為括號內的參數。在這個例子中，函數f接受三個輸入變量，並將它們相加得到輸出結果。",
    "chapter": "Technical Background"
  },
  {
    "question": "What is the purpose of adding a column of ones in np.dot(xs_p1, w)?",
    "options": [
      "To align the dimensions for matrix multiplication", 
      "To change the shape of the output", 
      "To reduce computational complexity", 
      "To remove bias from the data"
    ],
    "answer": 0,
    "explanation": "在np.dot(xs_p1, w)中添加一列1的主要目的是為了對齊矩陣乘法的維度。在線性模型中，我們通常將模型表示為y = Xw，其中X是特徵矩陣，w是權重向量。添加一列1（通常被稱為截距項或偏置項）允許模型學習一個常數偏移，使線性模型能夠擬合數據的整體水平。從數學角度看，這相當於將模型從y = w₁x₁ + w₂x₂ + ... + wₙxₙ擴展為y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ，其中w₀是偏置項。添加這一列確保了矩陣乘法能夠正確執行，同時使模型更加靈活。",
    "chapter": "Technical Background"
  },
  {
    "question": "What happens when we swap the order of np.dot(row_vec, col_vec) and np.dot(col_vec, row_vec)?",
    "options": [
      "Both produce the same scalar output", 
      "One produces a scalar, and the other produces a matrix", 
      "Both result in a shape mismatch error", 
      "NumPy automatically converts them to the correct form"
    ],
    "answer": 1,
    "explanation": "當交換np.dot(row_vec, col_vec)和np.dot(col_vec, row_vec)的順序時，結果會有顯著不同。假設row_vec的形狀是(1,n)，col_vec的形狀是(n,1)：\n\n- np.dot(row_vec, col_vec)的結果是一個標量（單個數值），因為(1,n)和(n,1)的矩陣乘法產生形狀為(1,1)的結果，即一個標量。\n- np.dot(col_vec, row_vec)的結果是一個矩陣，形狀為(n,n)，因為(n,1)和(1,n)的矩陣乘法產生一個n×n的矩陣。\n\n這種差異源於矩陣乘法的性質：若A的形狀是(m,n)，B的形狀是(n,p)，則A·B的形狀是(m,p)。交換順序改變了結果的維度和性質。",
    "chapter": "Technical Background"
  },
  {
    "question": "Which of the following statements is true about classification tasks?",
    "options": [
      "Binary classification refers to problems with more than two possible target classes.", 
      "Classification problems can be either binary or multiclass, depending on the number of target classes.", 
      "Naive Bayes directly assigns an output class without intermediate probability estimation.", 
      "The encoding of binary classification outputs as {-1, +1} or {0, 1} significantly impacts the model's performance."
    ],
    "answer": 1,
    "explanation": "關於分類任務，正確的陳述是：分類問題可以是二元（binary）或多類（multiclass），這取決於目標類別的數量。二元分類處理的是只有兩個可能目標類別的問題（例如垃圾郵件檢測：是/否），而多類分類處理的是有三個或更多可能目標類別的情況（例如動物種類識別：狗/貓/鳥等）。其他選項都不正確：二元分類是指只有兩個可能類別的問題，不是多於兩個；朴素貝葉斯確實計算中間概率；而二元分類輸出的編碼方式（{-1, +1}或{0, 1}）通常不會顯著影響模型性能，雖然可能會影響某些算法的實現細節。",
    "chapter": "Classification"
  },
  {
    "question": "Why is \"teaching to the test\" considered a bad practice in machine learning?",
    "options": [
      "It leads to a lower training error.", 
      "It results in a model that performs well only on the training data but poorly on unseen data.", 
      "It ensures that the model generalizes well to new problems.", 
      "It reduces the need for a separate test dataset."
    ],
    "answer": 1,
    "explanation": "在機器學習中，「教向測試」（teaching to the test）被視為不良實踐，因為它會導致模型僅在訓練數據上表現良好，但在未見過的數據上表現不佳。這實際上是過擬合（overfitting）的一種形式，模型過度適應了訓練數據的具體特點，包括其中的噪聲和特殊模式，而失去了泛化能力。這種做法會導致模型在實際應用中表現不佳，因為真實世界的數據往往與訓練數據有所不同。良好的機器學習實踐應該注重模型在未見過的數據上的泛化能力，而不只是在訓練集上獲得高分數。",
    "chapter": "Classification"
  },
  {
    "question": "What is the primary purpose of using train_test_split in machine learning?",
    "options": [
      "To separate categorical and numerical features in the dataset.", 
      "To split the dataset into training and testing subsets for evaluating generalization.", 
      "To remove outliers from the dataset before training.", 
      "To randomly shuffle the dataset before visualization."
    ],
    "answer": 1,
    "explanation": "在機器學習中使用train_test_split的主要目的是將數據集分割成訓練和測試子集，以評估模型的泛化能力。訓練集用於訓練模型，而測試集用於評估模型對未見過數據的表現。這種分割允許我們模擬模型在實際應用中可能遇到的情況，確保模型不僅能記住訓練數據，還能對新數據做出準確預測。如果模型在訓練數據上表現良好但在測試數據上表現差，這可能表明模型存在過擬合問題。因此，訓練/測試分割是評估模型性能和可靠性的關鍵步驟。",
    "chapter": "Classification"
  },
  {
    "question": "What does accuracy measure in the evaluation of a machine learning model?",
    "options": [
      "The total number of predictions made by the model.", 
      "The proportion of correctly classified instances out of all instances.", 
      "The difference between the highest and lowest predicted values.", 
      "The number of features used in the classification model."
    ],
    "answer": 1,
    "explanation": "在機器學習模型評估中，準確率（accuracy）衡量的是正確分類的實例佔所有實例的比例。它通過將模型正確預測的實例數量除以總實例數量來計算。準確率是一個介於0和1（或0%和100%）之間的值，其中1（或100%）表示完美的分類性能，而0（或0%）表示完全錯誤的分類。雖然準確率是一個直觀且常用的評估指標，但在處理不平衡類別分布或不同類型的錯誤具有不同成本的情況時，它可能會產生誤導。在這些情況下，可能需要使用其他評估指標，如精確度（precision）、召回率（recall）、F1分數或混淆矩陣（confusion matrix）進行補充。",
    "chapter": "Classification"
  },
  {
    "question": "What is the fundamental idea behind the nearest-neighbors algorithm?",
    "options": [
      "Predicting the class of a new example based on the most similar known example(s).", 
      "Assigning random labels to new examples to test model robustness.", 
      "Using deep learning to classify data based on hidden layers.", 
      "Averaging all feature values in the dataset to determine a prediction."
    ],
    "answer": 0,
    "explanation": "最近鄰（nearest-neighbors）算法的基本思想是根據最相似的已知示例來預測新示例的類別。這個算法假設相似的數據點傾向於屬於相同的類別。工作原理是：對於一個新的數據點，算法計算它與訓練集中所有數據點之間的距離，找出最近的一個或多個鄰居，然後基於這些鄰居的類別來預測新數據點的類別（例如，通過多數投票）。這是一種直觀的方法，基於這樣的假設：在特徵空間中彼此接近的數據點很可能屬於相同的類別。最近鄰方法是非參數的，意味著它不對數據做出特定的假設，而是讓數據本身決定模型的結構。",
    "chapter": "Classification"
  },
  {
    "question": "Which of the following is a valid way to measure similarity between examples in a nearest-neighbors classifier?",
    "options": [
      "Counting the number of missing values in the dataset.", 
      "Using a distance metric such as Euclidean or Hamming distance.", 
      "Assigning similarity based on the order of data in the dataset.", 
      "Choosing a random similarity score for each pair of examples."
    ],
    "answer": 1,
    "explanation": "在最近鄰分類器中，有效測量示例之間相似度的方法是使用距離度量，如歐幾里得距離或漢明距離。這些距離度量提供了一種量化特徵空間中數據點之間相似度的方法：\n\n- 歐幾里得距離：測量多維空間中兩點之間的直線距離，適用於連續數值特徵。\n- 漢明距離：測量兩個等長向量或字符串之間不同位置的數量，適用於分類或二元特徵。\n\n選擇合適的距離度量對最近鄰算法的性能至關重要，因為它直接影響如何識別「最近」的鄰居。距離越小，兩個數據點被認為越相似。還有其他距離度量也可用於特定類型的數據，如曼哈頓距離、餘弦相似度或馬氏距離。",
    "chapter": "Classification"
  },
  {
    "question": "What does the \"k\" in k-Nearest Neighbors (k-NN) represent?",
    "options": [
      "The number of features used for classification.", 
      "The number of neighbors considered when making a prediction.", 
      "The number of clusters in the dataset.", 
      "The number of training samples in the dataset."
    ],
    "answer": 1,
    "explanation": "在k-最近鄰（k-Nearest Neighbors，k-NN）算法中，\"k\"代表做出預測時考慮的鄰居數量。當分類一個新的數據點時，k-NN算法會找出訓練集中距離該點最近的k個數據點，然後通過這k個鄰居的多數投票（對於分類）或平均值（對於回歸）來決定新數據點的預測結果。\n\nk值是一個重要的超參數，它影響模型的複雜度和性能：\n- 較小的k值使模型更複雜，可能導致過擬合，對噪聲敏感。\n- 較大的k值使模型更簡單，可能導致欠擬合，忽略局部模式。\n\n選擇適當的k值通常需要使用交叉驗證等技術進行調優，以在模型複雜度和泛化能力之間找到平衡。",
    "chapter": "Classification"
  },
  {
    "question": "How does k-NN classify a new data point?",
    "options": [
      "By using a deep neural network to extract features.", 
      "By randomly assigning a label based on the training set distribution.", 
      "By calculating the similarity between data points and choosing the most frequent class among the k-nearest neighbors.", 
      "By fitting a parametric equation to the dataset before classification."
    ],
    "answer": 2,
    "explanation": "k-NN（k-最近鄰）算法通過計算數據點之間的相似度並選擇k個最近鄰中最頻繁的類別來分類新數據點。具體步驟如下：\n\n1. 計算新數據點與訓練集中所有數據點之間的距離（使用歐氏距離、曼哈頓距離等）\n2. 根據距離排序，選出最近的k個鄰居\n3. 對於分類問題，採用多數投票機制，選擇這k個鄰居中出現頻率最高的類別作為新數據點的預測類別\n4. 對於回歸問題，計算這k個鄰居目標值的平均值作為預測結果\n\nk-NN是一種懶惰學習（lazy learning）算法，因為它在訓練階段不學習任何模型，而是在預測階段直接使用訓練數據。它的優點是簡單直觀，不需要假設數據分布，但計算成本高且容易受到「維度詛咒」的影響。",
    "chapter": "Classification"
  },
  {
    "question": "Why is k-NN considered a nonparametric learning method?",
    "options": [
      "Because it uses a fixed number of parameters to learn from data.", 
      "Because it transforms input data into a new feature space before classification.", 
      "Because it requires hyperparameter tuning before training.", 
      "Because it does not make assumptions about the form of the relationship between features and targets."
    ],
    "answer": 3,
    "explanation": "k-NN（k-最近鄰）被視為非參數學習方法，因為它不對特徵和目標之間的關係形式做出假設。與參數模型（如線性回歸）不同，參數模型假設數據遵循特定的數學函數形式並估計固定數量的參數，k-NN不學習固定的模型結構或參數集。\n\n相反，k-NN直接使用訓練數據本身作為模型，根據新數據點與訓練實例的相似度進行預測。它讓數據自己「說話」，沒有預設的函數形式約束其預測能力。這種靈活性使k-NN能夠適應複雜的決策邊界，但代價是需要存儲整個訓練集，且預測時計算成本較高。\n\n非參數方法的主要優勢是它們可以適應各種數據分布，而不需要事先知道或假設數據的結構。",
    "chapter": "Classification"
  },
  {
    "question": "What is the key assumption made by the Naive Bayes classifier?",
    "options": [
      "All features are conditionally independent given the class label.", 
      "All features have the same distribution in every class.", 
      "The model does not require any training data.", 
      "The features are always dependent on one another."
    ],
    "answer": 0,
    "explanation": "朴素貝葉斯（Naive Bayes）分類器的關鍵假設是：在給定類別標籤的條件下，所有特徵相互獨立。這種假設被稱為「朴素」（naive），因為在現實世界的數據中，特徵通常是相互關聯的，而不是完全獨立的。\n\n這一假設使得我們可以將條件概率簡化為各個特徵條件概率的乘積：P(x₁,x₂,...,xₙ|y) = P(x₁|y) × P(x₂|y) × ... × P(xₙ|y)，其中x₁到xₙ是特徵，y是類別標籤。這種簡化大大減少了需要估計的參數數量，使模型在有限的訓練數據上也能有效學習。\n\n儘管特徵獨立性的假設在實際情況中很少完全成立，但朴素貝葉斯在許多應用中（如文本分類、垃圾郵件過濾）表現出色，證明了它對這種假設違反的魯棒性。",
    "chapter": "Classification"
  },
  {
    "question": "How does Naive Bayes classify a new example?",
    "options": [
      "By finding the k-nearest neighbors and selecting the most common class.", 
      "By fitting a linear function to separate the classes.", 
      "By computing the probability of each class given the feature values and choosing the most likely class.", 
      "By randomly assigning class labels based on prior probabilities."
    ],
    "answer": 2,
    "explanation": "朴素貝葉斯通過計算給定特徵值條件下每個類別的概率，然後選擇最可能的類別來對新示例進行分類。具體步驟如下：\n\n1. 利用貝葉斯定理，計算P(y|X) ∝ P(X|y) × P(y)，其中y是類別，X是特徵向量\n2. 根據條件獨立性假設，簡化為P(y|X) ∝ P(y) × P(x₁|y) × P(x₂|y) × ... × P(xₙ|y)\n3. 對每個可能的類別y計算這個概率\n4. 選擇具有最高概率的類別作為預測結果\n\n在訓練階段，模型從訓練數據中學習先驗概率P(y)和條件概率P(xᵢ|y)。這些參數通常使用最大似然估計或添加平滑處理（如拉普拉斯平滑）的變體來計算。\n\n朴素貝葉斯的計算效率高，適合處理高維特徵空間，且在小樣本數據上仍能表現良好，因此在文本分類等應用中特別流行。",
    "chapter": "Classification"
  },
  {
    "question": "Why is Naive Bayes particularly effective for text classification?",
    "options": [
      "It ignores the relationships between words, allowing it to focus on individual word frequencies.", 
      "It requires a deep understanding of grammar and syntax.", 
      "It can only classify texts with short sentences.", 
      "It performs complex deep learning operations on text data."
    ],
    "answer": 0,
    "explanation": "朴素貝葉斯之所以在文本分類中特別有效，是因為它忽略了詞語之間的關係，專注於個別詞頻。這種特性恰好適合處理文本數據的以下特點：\n\n1. 高維稀疏性：文本通常表示為高維向量（詞袋模型），大多數文檔只包含詞彙表中的一小部分詞語。朴素貝葉斯能有效處理這種稀疏特徵。\n\n2. 獨立性假設與詞袋模型匹配：忽略詞序和語法，只考慮詞頻的詞袋表示與朴素貝葉斯的條件獨立假設相符。\n\n3. 計算效率：處理大型文本語料庫時，其簡單的計算模型非常高效。\n\n4. 樣本效率：即使訓練數據有限，也能學習有效的分類器。\n\n儘管完全忽略詞序和語法結構看似過於簡化，但在實踐中，朴素貝葉斯在垃圾郵件過濾、情感分析和文檔分類等文本分類任務中仍能取得驚人的好結果。",
    "chapter": "Classification"
  },
  {
    "question": "What is the main reason to standardized (z-scored) the dataset?",
    "options": [
      "To remove any missing data points from the dataset.", 
      "To encode categorical variables into numerical values.", 
      "To ensure all features have a mean of 0 and a standard deviation of 1, reducing bias from differing feature scales.", 
      "To visually display relationships between variables using pairplots."
    ],
    "answer": 2,
    "explanation": "標準化（z-score標準化）數據集的主要原因是確保所有特徵都有0均值和1標準差，從而減少不同特徵尺度差異帶來的偏差。這一處理在機器學習中非常重要，原因有以下幾點：\n\n1. 消除尺度影響：不同特徵可能有不同的測量單位和範圍（例如，年齡可能是0-100，而收入可能是幾萬到幾百萬）。這些尺度差異會導致某些特徵在模型中不成比例地產生影響。\n\n2. 改善算法性能：許多機器學習算法（如梯度下降、支持向量機和神經網絡）在特徵標準化後表現更好，收斂更快。\n\n3. 特徵可比性：標準化後，所有特徵的重要性可以在同一尺度上進行比較。\n\n4. 正則化效果：對於使用L1或L2正則化的模型，標準化確保懲罰項對所有特徵的影響相等。\n\n標準化的公式為：z = (x - μ) / σ，其中x是原始值，μ是特徵的均值，σ是特徵的標準差。",
    "chapter": "Regression"
  },
  {
    "question": "Which statement correctly describes the arithmetic mean?",
    "options": [
      "It is the middle value when data points are ordered.", 
      "It balances the count of values above and below it.", 
      "It is robust to extreme values or outliers.", 
      "It balances the total distances of values on either side."
    ],
    "answer": 3,
    "explanation": "正確描述算術平均數（arithmetic mean）的陳述是：它平衡了各值在兩側的總距離。算術平均數是數據點總和除以數據點數量，它具有一個重要的數學性質：數據點與平均數的偏差總和等於零。換句話說，數據點高於平均數的總量正好等於低於平均數的總量，因此平均數平衡了數據在其兩側的總距離。\n\n其他選項描述的是不同的統計量：\n- 「有序數據中的中間值」描述的是中位數（median）\n- 「平衡上下值的計數」也是描述中位數的特性\n- 「對極端值或異常值具有魯棒性」是中位數的特點，而不是平均數，因為平均數容易受極端值影響\n\n算術平均數在統計和數據分析中廣泛使用，提供了數據中心趨勢的度量，但在存在極端異常值的情況下，可能不是最理想的中心趨勢度量。",
    "chapter": "Regression"
  },
  {
    "question": "What advantage does the median have over the mean when summarizing data?",
    "options": [
      "It always provides a higher numeric summary than the mean.",
      "It accounts for the exact numeric values of data points.",
      "It remains stable even when extreme values or outliers occur.",
      "It can be easily weighted based on data point distances."
    ],
    "answer": 2,
    "explanation": "中位數在摘要資料時的主要優勢是它對極端值或異常值具有魯棒性，也就是說，就算資料中有離群值，中位數也不太會受到影響。相較之下，平均數會因為極端大或小的值而產生偏移。\n\n選項解析：\n- A 錯誤：中位數不一定比平均數大，小、相等都有可能，取決於資料分佈。\n- B 錯誤：「考慮精確數值」這是平均數的特性，不是中位數。\n- C 正確：中位數是排序後的中間值，對極端值不敏感。\n- D 錯誤：「根據距離加權」這不是中位數的特性，這種說法比較接近加權平均。",
    "chapter": "Regression"
  },
  {
    "question": "In the context of building a k-NN regression model using sklearn, which of the following statements is correct?",
    "options": [
      "The evaluation metric used for k-NN regression is accuracy_score.",
      "The k-NN regression model predicts Boolean categorical values.",
      "The mean squared error (MSE) measures how far predictions deviate numerically from the actual targets.",
      "The KNeighborsClassifier is used instead of KNeighborsRegressor for regression tasks."
    ],
    "answer": 2,
    "explanation": "在 sklearn 中使用 k-NN 做迴歸時，常用的評估指標是 MSE（均方誤差），它用來衡量模型預測值與實際值之間的數值偏差。\n\n選項解析：\n- A 錯誤：accuracy_score 是分類任務的評估指標，不適用於迴歸。\n- B 錯誤：k-NN regression 模型預測的是連續數值，而不是布林分類。\n- C 正確：MSE 是用來量化預測與實際目標之間數值差異的指標。\n- D 錯誤：KNeighborsClassifier 用於分類任務，而不是迴歸。",
    "chapter": "Regression"
  },
  {
    "question": "In linear regression, why is a horizontal line placed at the mean of the data points often considered the 'best' line?",
    "options": [
      "It maximizes the sum of squared errors (SSE).",
      "It ensures all errors are positive values.",
      "It balances the vertical errors evenly, minimizing the total squared distance to all points.",
      "It provides the simplest visualization without considering errors."
    ],
    "answer": 2,
    "explanation": "在簡單線性回歸中，沒有任何斜率的假設下，將一條水平線放在平均值處，可以讓所有點的垂直距離平方總和（SSE）最小化，這是最佳線的數學定義。\n\n選項解析：\n- A 錯誤：目標是最小化 SSE，不是最大化。\n- B 錯誤：誤差可以是正的也可以是負的，這選項誤導。\n- C 正確：這條線讓點到線的垂直誤差平方總和最小，是最符合最佳線定義的。\n- D 錯誤：可視化不是重點，重點在於誤差最小。",
    "chapter": "Regression"
  },
  {
    "question": "When comparing different lines in linear regression, how is the 'best' tilted line typically selected?",
    "options": [
      "By choosing the line with the highest sum of errors.",
      "By choosing the line with the smallest sum of squared errors (SSE).",
      "By choosing the line whose slope (m) is zero.",
      "By choosing the line that always passes through the origin."
    ],
    "answer": 1,
    "explanation": "在迴歸中選出最佳傾斜直線的方法，是選擇 SSE（誤差平方總和）最小的那一條，這是最小平方法（least squares）的核心原則。\n\n選項解析：\n- A 錯誤：我們想最小化誤差總和，而不是最大化。\n- B 正確：SSE 越小代表預測越準確。\n- C 錯誤：斜率為 0 就是水平線，這未必是最適合的線。\n- D 錯誤：通過原點沒有特別意義，反而可能讓模型失準。",
    "chapter": "Regression"
  },
  {
    "question": "In linear regression with multiple features, why do we use weights instead of a single slope?",
    "options": [
      "Because multiple features each contribute separately to the prediction, requiring different slopes.",
      "Because the prediction no longer uses numerical features.",
      "Because using weights automatically removes prediction errors.",
      "Because multiple features mean we must always use the same slope for every feature."
    ],
    "answer": 0,
    "explanation": "多變數線性迴歸中，每個特徵對預測結果都有不同的貢獻，因此需要為每個特徵設置不同的權重（斜率），而不是用同一個。\n\n選項解析：\n- A 正確：這是線性迴歸的基本概念，每個變數都有各自的斜率。\n- B 錯誤：我們仍然使用數值特徵。\n- C 錯誤：使用權重不會自動消除預測誤差。\n- D 錯誤：每個特徵有不同的影響力，不能共用一個斜率。",
    "chapter": "Regression"
  },
  {
    "question": "According to the optimization strategies described, which of the following methods directly provides a mathematical shortcut to find the best solution for minimizing the sum of squared errors (SSE)?",
    "options": [
      "Random guess",
      "Random step",
      "Smart step",
      "Calculated shortcut"
    ],
    "answer": 3,
    "explanation": "計算捷徑（Calculated shortcut）指的是線性代數方法，如封閉解（Closed-form solution），可直接用數學公式快速計算最佳權重，而不需逐步優化。\n\n選項解析：\n- A 錯誤：隨機猜測太隨便，不會得出最佳解。\n- B 錯誤：隨機步伐是隨機搜尋，不精確。\n- C 錯誤：聰明步伐類似於梯度下降，但不是直接解。\n- D 正確：用數學捷徑可直接求出最小 SSE 解。",
    "chapter": "Regression"
  },
  {
    "question": "Why is Root Mean Squared Error (RMSE) a useful evaluation metric in regression tasks?",
    "options": [
      "Because it ensures positive and negative errors cancel each other out.",
      "Because it directly provides the sum of the errors without considering their magnitude.",
      "Because it measures prediction errors on the same scale as the original data, accounting for both direction and magnitude.",
      "Because it identifies the total number of accurate predictions in a regression model."
    ],
    "answer": 2,
    "explanation": "RMSE 是一個很實用的回歸評估指標，因為它的單位與原始資料相同（不像 MSE 是平方），可以清楚地看出預測值與實際值的平均偏差量，也能反映誤差的大小與方向。\n\n選項解析：\n- A 錯誤：這是說 MAE 或總誤差時可能會提到的性質，但 RMSE 是平方後開根號。\n- B 錯誤：這比較像是單純的總誤差，RMSE 有考慮誤差大小。\n- C 正確：RMSE 回到原始單位，讓誤差量更直觀可解釋。\n- D 錯誤：準確預測數量通常是分類任務的評估方式。",
    "chapter": "Regression"
  }
]
